[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "À propos",
    "section": "",
    "text": "À propos"
  },
  {
    "objectID": "equations_de_mouvement.html",
    "href": "equations_de_mouvement.html",
    "title": "Équation du mouvement, intégration numérique",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nDans cette leçon, nous verrons comment résoudre numériquement les équations du mouvement issues du principe fondamental de la dynamique."
  },
  {
    "objectID": "equations_de_mouvement.html#seconde-loi-de-newton",
    "href": "equations_de_mouvement.html#seconde-loi-de-newton",
    "title": "Équation du mouvement, intégration numérique",
    "section": "1 Seconde loi de Newton",
    "text": "1 Seconde loi de Newton\nOn considère un corpuscule, ou une particule, de masse \\(m\\), position \\(\\vec{r}\\) et vitesse \\(\\vec{v}\\) définis au temps \\(t\\). Sous l’action d’une force \\(\\vec{f}\\), la quantité de mouvement de la particule \\(\\vec{p} = m\\vec{v}\\) change selon la seconde loi de Newton : \\[ \\deriv{\\vec{p}}{t} = \\vec{f}, \\] ou, si l’on suppose la masse constante : \\[ m\\deriv{\\vec{v}}{t} = \\vec{f}.\\]\nLa seconde loi de Newton se généralise très bien à un système à \\(N\\) particules. On notera \\(\\{\\vec{r}_i\\}_{i=1}^N\\) l’ensemble des positions, \\(\\{\\vec{v}_i\\}_{i=1}^N\\) l’ensemble des vitesses, \\(\\{m_i\\}_{i=1}^N\\) l’ensemble des masses et \\(\\{\\vec{f}_i\\}_{i=1}^N\\) l’ensemble des forces totales agissant sur chacune des particules. Nous verrons que le calcul de \\(\\vec{f}_i\\) pour chaque particule est central dans les méthodes de simulations discrètes, puisque \\(\\vec{f}_i\\) contrôle le mouvement de la particule \\(i\\). On peut alors écrire la seconde loi de Newton pour un système de \\(N\\) particules :\n\\[ m_i\\frac{\\mathrm{d}\\vec{v}_i}{\\mathrm{d}t} = \\vec{f}_i. \\tag{1}\\]"
  },
  {
    "objectID": "equations_de_mouvement.html#discrétisation",
    "href": "equations_de_mouvement.html#discrétisation",
    "title": "Équation du mouvement, intégration numérique",
    "section": "2 Discrétisation",
    "text": "2 Discrétisation\nLe problème à trois corps est un problème de physique classique où trois particules interagissent par l’attraction gravitationnelle. Dans le cas général, ce problème n’est pas soluble analytiquement1, mais on peut le résoudre de manière approchée par des techniques numériques.\nLes techniques de résolution numérique sont essentiellement basées sur un principe : celui de transformer une équation différentielle en un ensemble d’équations algébriques (qui ne font pas intervenir la dérivée). C’est le procédé de discrétisation. Cependant, ce procédé a un coût, celui de n’obtenir qu’une solution approchée du problème original. L’erreur commise, appelée erreur de discrétisation, est souvent inévitable, mais on peut la contrôler dans beaucoup de cas.\nOn va discuter dans cette leçon de différentes façons d’approximer la solution d’une équation différentielle, et d’une méthode acceptable pour la solution approchée de la seconde loi de Newton.\n\n2.1 Partition du temps\nLa première étape du processus de discrétisation est de sélectionner des valeurs discrètes de la variable de notre équation différentielle, ici le temps. Dans le problème original \\(t \\in [0, +\\infty)\\). On défini donc un ensemble de \\(M\\) temps discrets : \\[t_i = \\Delta t\\cdot i,\\quad i \\in [0, M-1],\\] ainsi les temps \\(t_i\\) sont uniformément espacés de \\(\\Delta t\\), que l’on appellera pas de temps. On verra que cette quantité contrôle bon nombre de propriétés numériques de notre système (les propriétés numériques sont uniquement dépendantes des choix de discrétisation, et sont à distinguer des propriétés physiques du système, qui dépendent des équations gouvernant le système)."
  },
  {
    "objectID": "equations_de_mouvement.html#intégration-en-temps-premier-ordre",
    "href": "equations_de_mouvement.html#intégration-en-temps-premier-ordre",
    "title": "Équation du mouvement, intégration numérique",
    "section": "3 Intégration en temps – premier ordre",
    "text": "3 Intégration en temps – premier ordre\nAvant de discuter de l’équation de Newton, intéressons-nous à une équation différentielle plus simple :\n\\[ \\dot{y} + y = 0,\\quad y(0) = 1, \\]\ndont on connait la solution, \\(y(t) = e^{-t}\\). Posons l’équation pour les temps discrets \\(t_i\\). On introduit la notation \\(y_i = y(t_i)\\):\n\\[ \\dot{y}_i + y_i = 0,\\quad i\\in[0, M-1]. \\]\n\n3.1 Schéma d’Euler explicite\nPour trouver les valeurs inconnues \\(y_i\\), il nous faut la dérivée de \\(y\\), que l’on ne connait pas. En revanche, on peut l’approximer par différence finie :\n\\[ \\dot{y}_i \\approx \\frac{y(t_{i+1}) - y(t_i)}{t_{i+1} - t_i} = \\frac{y_{i+1} - y_i}{\\Delta t} \\]\nDe toute évidence l’approximation devient exacte quand \\(\\Delta t \\rightarrow 0\\). On peut remplacer notre approximation dans l’équation différentielle :\n\\[ \\frac{y_{i+1} - y_i}{\\Delta t} + y_i = 0 \\quad \\Leftrightarrow \\quad y_{i+1} = (1 - \\Delta t)y_i,\\quad i\\in[0, M-1] \\]\nOn a, par approximation de la dérivée en différences finies, transformé une équation différentielle en \\(M\\) équations algébriques, qui définissent en fait une relation de récurrence pour la suite \\(\\{y_i\\}_{i=0}^{M-1}\\). Puisque la relation de récurrence définit une suite géométrique, on peut écrire \\(y_i\\) directement :\n\\[ y_i = (1 - \\Delta t)^i y_0,\\] on remarque alors une propriété particulière : si \\(\\Delta t &lt; 1\\) la limite de la suite est nulle et la suite est monotone, comme la solution analytique, mais si \\(2 &gt; \\Delta t &gt; 1\\) la suite oscille (si \\(\\Delta = 1\\) la suite est nulle) et si \\(\\Delta t &gt; 2\\) la suite diverge. C’est l’exemple parfait d’une propriété numérique, ici que \\(\\Delta t = 1\\) est un pas de temps critique qui conditionne la bonne marche de la résolution numérique. C’est une caractéristique du schéma d’Euler explicite qui est celui que l’on vient de mettre en place : il est conditionnellement stable.\n\n\n\n\n\n\nExercice\n\n\n\nPassons à l’expérience numérique : on implémente le schéma d’Euler explicite et on montre l’effet du pas de temps. Pour cela, on reformule les équations algébriques ci-dessus sous forme d’algorithme :\n\nRésolution de l’équation différentielle discrétisée pour trouver la dérivée : \\(\\dot{y}_{i} \\gets -y_i\\)\nMise-à-jour de la variable \\(y_{i+1} \\gets y_{i} + \\Delta t \\dot{y}_i\\)\n\\(i \\gets i+1\\)\n\nImplémentez l’algorithme ci-dessus et essayez de reproduire la Figure 1, qui montre le résultat de calculs avec différents pas de temps \\(\\Delta t\\). La solution de l’exercice est dans la cellule de code ci-dessous. Commentez les comportements montrées par la Figure 1.\n\n\n\n\n\nCode\ndef euler_explicit(t_final, dt):\n    M = int(t_final / dt) + 1\n    y = np.zeros(M)\n    t = np.arange(M) * dt\n    y[0] = 1\n\n    for i in range(M-1):\n        y_dot = -y[i]\n        y[i+1] = y[i] + dt * y_dot\n    return t, y\n\nt = np.linspace(0, 9, 200)\ny_exact = np.exp(-t)\n\ncmap = plt.get_cmap('copper')\n\nfig, ax = plt.subplots()\n\nax.plot(t, y_exact, label=\"Solution\", lw=4, color='k')\n\nfor dt in [0.5, 1, 1.5, 2.1]:\n    ax.plot(*euler_explicit(t.max(), dt), label=f\"$\\Delta t = {dt:.1f}$\", color=cmap(dt / 2))\n\nax.legend()\nax.set(xlabel=\"$t$\",\n       ylabel=\"$y(t)$\")\n\nplt.show()\n\nFigure 1: ?(caption)\n\n\n\n\n3.2 Schéma d’Euler implicite\nAu lieu de poser l’équation au temps \\(t_i\\), pour lequel on connaît la valeur de \\(y_i\\) (puisqu’on la calcule à l’itération précédente), posons l’équation au temps \\(t_{i+1}\\) auquel on ne connaît pas encore \\(y_{i+1}\\) (c’est pour cela que le schéma est implicite) : \\[ \\dot{y}_{i+1} + y_{i+1} = 0,\\] on doit à présent approximer \\(\\dot y_{i+1}\\) par différences finies : \\[ \\dot{y}_{i+1} \\approx \\frac{y(t_{i+1}) - y(t_i)}{t_{i+1} - t_i} = \\frac{y_{i+1} - y_i}{\\Delta t}, \\] en remplaçant dans l’équation différentielle : \\[ \\frac{y_{i+1} - y_i}{\\Delta t} + y_{i+1} = 0 \\quad \\Leftrightarrow \\quad y_{i+1} = \\frac{1}{1 + \\Delta t}y_i,\\quad i\\in[0, M-1] \\] Au premier abord, on dirait une situation similaire au schéma précédent : on a une relation de récurrence qui définit une suite géométrique. En revanche, on voit que pour tout \\(\\Delta t &gt; 0\\), la raison de la suite \\(1/(1 + \\Delta t)\\) est plus petite que 1. Le schéma que l’on vient de poser, appelé Euler implicite est donc inconditionnellement stable.\n\n\n\n\n\n\nExercice\n\n\n\nFaisons la même expérience que précédemment, en formulant les équations sous forme d’algorithme. Comme on a une formulation implicite, l’écriture de l’algorithme est moins évidente. On doit partir de l’équation de différence finie pour \\(\\dot{y}_{i+1}\\). En combinant avec l’équation différentielle, on peut écrire que :\n\\[\\begin{align*}\n\\dot{y}_{i+1} (= -y_{i+1}) & = \\frac{y_{i+1} - y_i}{\\Delta t}\\\\\n\\Leftrightarrow y_{i+1} & = y_i + \\Delta t \\dot{y}_{i+1}\\\\\n\\Leftrightarrow -\\dot{y}_{i+1} &= y_i + \\Delta t\\dot{y}_{i+1}\\\\\n\\Leftrightarrow \\dot{y}_{i+1} &= \\frac{-1}{1 + \\Delta t} y_i\n\\end{align*}\\]\nOn peut alors écrire l’agorithme suivant:\n\n\\(\\dot{y}_{i+1}\\gets \\frac{-1}{1 + \\Delta t} y_i\\)\n\\(y_{i+1} \\gets y_i + \\Delta t \\dot{y}_{i+1}\\)\n\\(i\\gets i + 1\\)\n\nImplémentez l’algorithme ci-dessus et essayez de reproduire la Figure 2, qui montre des calculs par schéma d’Euler implicite pour différents pas de temps. Commentez la différence avec la Figure 1.\n\n\n\n\n\nCode\ndef euler_implicit(t_final, dt):\n    M = int(t_final / dt) + 1\n    y = np.zeros(M)\n    t = np.arange(M) * dt\n    y[0] = 1\n\n    for i in range(M-1):\n        y_dot = -1 / (1 + dt) * y[i]\n        y[i+1] = y[i] + dt * y_dot\n    return t, y\n\nt = np.linspace(0, 8, 200)\ny_exact = np.exp(-t)\n\nplt.plot(t, y_exact, label=\"Solution\", lw=4, color='k')\n\nfor dt in [0.5, 1, 1.5, 2.1]:\n    plt.plot(*euler_implicit(t.max(), dt), label=f\"$\\Delta t = {dt:.1f}$\", color=cmap(dt / 2))\n\nplt.legend()\nplt.xlabel(\"$t$\")\nplt.ylabel(\"$y(t)$\")\nplt.show()\n\nFigure 2: ?(caption)\n\n\nOn observe, certes, que plus \\(\\Delta t\\) est grand, moins la solution numérique est proche de la solution analytique, mais aucune solution numérique n’a de comportement oscillant comme précédemment. On voit également, contrairement à la figure d’Euler explicite, que la pente initiale est fausse pour les solutions approximées.\n\n\n3.3 Améliorer la convergence\nOn a vu deux façons de mettre à jour une variable avec la méthode d’Euler :\n\nEuler explicite, qui utilise \\(\\dot y_{i}\\) : \\(y_{i+1} \\gets y_i + \\Delta t y_i\\)\nEuler implicite, qui utilise \\(\\dot y_{i+1}\\) : \\(y_{i+1} \\gets y_i + \\Delta t y_{i+1}\\)\n\nLe schéma ci-dessus montre graphiquement ces deux façons de mettre à jour la variable :\n\n\n\nCode\nfig, ax = plt.subplots()\n\nstart = 1\nend = 2\ndt = 1\n\nt = np.linspace(start - 0.1, end + 0.1, 200)\ny = np.exp(-t)\n\nax.plot(t, y, color='k')\nax.plot([start, end], [np.exp(-start), np.exp(-end)], marker='o', color='r', ls='--')\n\n# Forward euler update\nax.axline((start, np.exp(-start)), slope=-np.exp(-start), color='orange', ls='--', lw=1.5)\nax.plot([end], [np.exp(-start) + dt * (-np.exp(-start))], marker='o', color='r')\n\n# Backward euler update\nax.axline((end, np.exp(-end)), slope=-np.exp(-end), color='gray', ls='--', lw=1.5)\nax.axline((start, np.exp(-start)), slope=-np.exp(-end), color='orange', ls='--', lw=1.5)\nax.plot([end], [np.exp(-start) + dt * (-np.exp(-end))], marker='o', color='r')\n\n\nax.set(xlabel='$t$', ylabel=\"$y(t)$\")\nplt.show()\n\nFigure 3: ?(caption)\n\n\nOn remarque qu’aucune des pentes utilisées pour la mise-à-jour n’est satisfaisante, et que pour atteindre le point rouge sur la courbe noire, la solution analytique, il faut faire un pas avec une pente entre la pente initiale \\(\\dot y_{i}\\) et la pente finale \\(\\dot y_{i+1}\\). On propose donc le schéma de mise à jour suivant, avec une interpolation linéaire des deux pentes :\n\\[ y_{n+1} \\gets y_i + \\Delta t(\\beta \\dot y_i + (1 - \\beta) \\dot y_{i+1}),\\quad \\beta \\in [0, 1]\\]\nNotons que pour \\(\\beta = 1\\) on a le schéma explicite et \\(\\beta = 0\\) le schéma implicite précédent, mais le schéma est implicite pour toute valeur de \\(\\beta\\) non-nulle, car il faut calculer \\(\\dot y_{i+1}\\). Résolvons pour l’inconnue \\(\\dot y_{i+1}\\), on obtient :\n\\[ \\dot y_{i+1} = \\frac{-1}{1 + \\Delta t(1 - \\beta)}(y_i + \\Delta t \\beta \\dot y_i) \\]\nOn peut alors écrire un algorithme sous la forme d’un schéma de prédiction-correction :\n\n\\(\\dot y_i \\gets -y_i\\)\n\\(y_\\mathrm{pred} \\gets y_i + \\Delta t\\beta \\dot y_i\\) (prédiction)\n\\(\\dot y_{i+1} \\gets \\frac{-1}{1 + \\Delta t(1 - \\beta)}y_\\mathrm{pred}\\)\n\\(y_{i+1} \\gets y_\\mathrm{pred} + \\Delta t(1-\\beta)\\dot y_{i+1}\\) (correction)\n\\(i \\gets i+1\\)\n\n\n\n\n\n\n\nImportant\n\n\n\nLa structure “prédiction \\(\\rightarrow\\) mise-à-jour de la dérivée \\(\\rightarrow\\) correction” est très utilisée par les schémas d’intégration en temps. C’est d’ailleurs cette structure que l’on utilisera pour résoudre l’Équation 1.\n\n\n\n\n\n\n\n\nExercice\n\n\n\nImplémentez l’algorithme ci-dessus, et essayez de reproduire la Figure 4, qui compare les trois schémas d’intégrations en temps (\\(\\beta = 0\\), \\(\\beta = 1\\) et \\(\\beta = 0.5\\)) pour deux valeurs de \\(\\Delta t\\).\n\n\n\n\n\nCode\ndef euler_partial(t_final, dt, beta):\n    M = int(t_final / dt) + 1\n    y = np.zeros(M)\n    t = np.arange(M) * dt\n    y[0] = 1\n\n    for i in range(M-1):\n        y_dot = -y[i]\n        y[i+1] = y[i] + dt * beta * y_dot\n        y_dot = -1 / (1 + dt * (1 - beta)) * y[i+1]\n        y[i+1] += dt * (1-beta) * y_dot\n    return t, y\n\nfig, axs = plt.subplots(1, 2, figsize=(8, 4))\n\nt = np.linspace(0, 8, 200)\ny_exact = np.exp(-t)\n\nfor ax in axs:\n    ax.plot(t, y_exact, label=\"Solution\", lw=4, color='k')\n\nfor ax, dt in zip(axs, [1.5, 0.5]):   \n    ax.plot(*euler_implicit(t.max(), dt), label=\"$\\\\beta = 0$\")\n    ax.plot(*euler_explicit(t.max(), dt), label=\"$\\\\beta = 1$\")\n    ax.plot(*euler_partial(t.max(), dt, 0.5), label=\"$\\\\beta = 0.5$\")\n    ax.legend()\n    ax.set(xlabel='$t$', ylabel='$y(t)$', title=f'$\\\\Delta t= {dt:.1f}$')\n\nfig.tight_layout()\nplt.show()\n\nFigure 4: ?(caption)\n\n\nOn remarque que l’approximation avec \\(\\beta = 0.5\\) est bien meilleures que les deux schémas précédents. En revanche, cela se fait au prix de deux désavantages :\n\nL’algorithme est implicite. Ce n’est pas très grave sur un système aussi simple, mais sur de grosses simulations le coût de calcul de \\(\\dot y_{i+1}\\) peut être élevé.\nMalgré le fait que l’algorithme soit implicite, il est conditionnellement stable : selon la valeur de \\(\\beta\\), \\(\\Delta t\\) ne peut pas dépasser une certaine valeur\n\nOn a donc gagné en précision en ayant le même coût de calcul que le schéma d’Euler implicite mais sans son avantage principal d’être inconditionnellement stable. C’est à la personne faisant la simulation de choisir l’algorithme adapté !"
  },
  {
    "objectID": "equations_de_mouvement.html#intégration-en-temps-second-ordre",
    "href": "equations_de_mouvement.html#intégration-en-temps-second-ordre",
    "title": "Équation du mouvement, intégration numérique",
    "section": "4 Intégration en temps — second ordre",
    "text": "4 Intégration en temps — second ordre"
  },
  {
    "objectID": "equations_de_mouvement.html#lectures-conseillées",
    "href": "equations_de_mouvement.html#lectures-conseillées",
    "title": "Équation du mouvement, intégration numérique",
    "section": "5 Lectures conseillées",
    "text": "5 Lectures conseillées\n\nBonnet et Frangi (2006), chapitre 9"
  },
  {
    "objectID": "equations_de_mouvement.html#footnotes",
    "href": "equations_de_mouvement.html#footnotes",
    "title": "Équation du mouvement, intégration numérique",
    "section": "Notes de bas de page",
    "text": "Notes de bas de page\n\n\nDes solutions particulières existent, mais le cas général est un parfait exemple de système chaotique↩︎"
  }
]